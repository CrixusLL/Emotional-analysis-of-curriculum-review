# -*- coding: utf-8 -*-
"""demo_refactored_dianping_classification_with_BERT_fastai.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mkYcrWcnGH6T9KyIhncUrSGkycodHkQG

<a href="https://colab.research.google.com/github/wshuyi/demo_chinese_text_classification_bert_fastai/blob/master/demo_refactored_dianping_classification_with_BERT_fastai.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

from fastai.text import *
from fastai.callbacks import *
from fastai.vision import *
from fastai import *
import pandas as pd

df = pd.read_csv("newtest.csv")

from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=.2, random_state=2)

train, valid = train_test_split(train, test_size=.2, random_state=2)

len(train)

len(valid)

len(test)

train.head(10)

!pip install pytorch-transformers

from pytorch_transformers import BertTokenizer, BertForSequenceClassification

bert_model = "bert-base-chinese"
max_seq_len = 128 #每句话最长不能超过128个字
batch_size = 32  #每次训练用32条数据作为一个批次

bert_tokenizer = BertTokenizer.from_pretrained(bert_model)

list(bert_tokenizer.vocab.items())[2000:2005]
#len(bert_tokenizer.vocab.items())

bert_vocab = Vocab(list(bert_tokenizer.vocab.keys()))

class BertFastaiTokenizer(BaseTokenizer):
    def __init__(self, tokenizer, max_seq_len=128, **kwargs):
        self.pretrained_tokenizer = tokenizer
        self.max_seq_len = max_seq_len

    def __call__(self, *args, **kwargs):
        return self

    def tokenizer(self, t):
        return ["[CLS]"] + self.pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + ["[SEP]"]

tok_func = BertFastaiTokenizer(bert_tokenizer, max_seq_len=max_seq_len)

bert_fastai_tokenizer = Tokenizer(
    tok_func=tok_func,
    pre_rules=[],
    post_rules=[]
    
)

import os
path = Path(".")

databunch = TextClasDataBunch.from_df(path, train, valid, test,
                  tokenizer=bert_fastai_tokenizer,
                  vocab=bert_vocab,
                  include_bos=False,
                  include_eos=False,
                  text_cols="comment",
                  label_cols='sentiment',
                  bs=batch_size,
                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),
                  pin_memory=True,
                  num_workers = 1,
                  device=torch.device("cuda")
             )

databunch.show_batch()

class MyNoTupleModel(BertForSequenceClassification):
  def forward(self, *args, **kwargs):
    return super().forward(*args, **kwargs)[0] #只取出来第一项作为结果使用,新的Transformer模型和原先版本的BERT预训练模型差异

bert_pretrained_model = MyNoTupleModel.from_pretrained(bert_model, num_labels=2)

loss_func = nn.CrossEntropyLoss()

learn = Learner(databunch, 
                bert_pretrained_model,
                loss_func=loss_func,
                metrics=accuracy)

learn.lr_find()

learn.recorder.plot()

learn.fit_one_cycle(5, 1e-6) #选择一个学习率高同时能保证较低误差的

def dumb_series_prediction(n):
  preds = []
  for loc in range(n):
    preds.append(int(learn.predict(test.iloc[loc]['comment'])[1]))
  return preds

preds = dumb_series_prediction(len(test))

preds[:10]

from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(test.sentiment, preds))

print(confusion_matrix(test.sentiment, preds))

