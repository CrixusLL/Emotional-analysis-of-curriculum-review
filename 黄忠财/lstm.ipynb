{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dfead16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 180, 20)           50940     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 99,542\n",
      "Trainable params: 99,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "191/191 [==============================] - 15s 69ms/step - loss: 0.3846 - accuracy: 0.8371\n",
      "Epoch 2/3\n",
      "191/191 [==============================] - 14s 73ms/step - loss: 0.2241 - accuracy: 0.9267\n",
      "Epoch 3/3\n",
      "191/191 [==============================] - 14s 75ms/step - loss: 0.1905 - accuracy: 0.9414\n",
      "新数据学习前模型在测试集上的准确率为: 0.911660777385159.\n",
      "Epoch 1/2\n",
      "51/51 [==============================] - 4s 69ms/step - loss: 0.2581 - accuracy: 0.9110\n",
      "Epoch 2/2\n",
      "51/51 [==============================] - 4s 69ms/step - loss: 0.1921 - accuracy: 0.9417\n",
      "新数据学习后模型在测试集上的准确率为: 0.9143109540636042.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 导入数据\n",
    "# 文件的数据中，特征为evaluation, 类别为label.\n",
    "def load_data(filepath, input_shape=20):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # 标签及词汇表\n",
    "    labels, vocabulary = list(df['label'].unique()), list(df['evaluation'].unique())\n",
    "\n",
    "    # 构造字符级别的特征\n",
    "    string = ''\n",
    "    for word in vocabulary:\n",
    "        string += word\n",
    "\n",
    "    vocabulary = set(string)\n",
    "\n",
    "    # 字典列表\n",
    "    word_dictionary = {word: i+1 for i, word in enumerate(vocabulary)}\n",
    "    with open('word_dict.pk', 'wb') as f:\n",
    "        pickle.dump(word_dictionary, f)\n",
    "    inverse_word_dictionary = {i+1: word for i, word in enumerate(vocabulary)}\n",
    "    label_dictionary = {label: i for i, label in enumerate(labels)}\n",
    "    with open('label_dict.pk', 'wb') as f:\n",
    "        pickle.dump(label_dictionary, f)\n",
    "    output_dictionary = {i: labels for i, labels in enumerate(labels)}\n",
    "\n",
    "    vocab_size = len(word_dictionary.keys()) # 词汇表大小\n",
    "    label_size = len(label_dictionary.keys()) # 标签类别数量\n",
    "\n",
    "    # 序列填充，按input_shape填充，长度不足的按0补充\n",
    "    x = [[word_dictionary[word] for word in sent] for sent in df['evaluation']]\n",
    "    x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
    "    y = [[label_dictionary[sent]] for sent in df['label']]\n",
    "    y = [np_utils.to_categorical(label, num_classes=label_size) for label in y]\n",
    "    y = np.array([list(_[0]) for _ in y])\n",
    "\n",
    "    return x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary\n",
    "\n",
    "# 创建深度学习模型， Embedding + LSTM + Softmax.\n",
    "def create_LSTM(n_units, input_shape, output_dim, filepath):\n",
    "    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size + 1, output_dim=output_dim,\n",
    "                        input_length=input_shape, mask_zero=True))\n",
    "    model.add(LSTM(n_units, input_shape=(x.shape[0], x.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(label_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    plot_model(model, to_file='./model_lstm.png', show_shapes=True)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# 模型训练\n",
    "def model_train(input_shape, filepath, model_save_path):\n",
    "\n",
    "    # 将数据集分为训练集和测试集，占比为9:1\n",
    "    # input_shape = 100\n",
    "    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath, input_shape)\n",
    "    train_x_first, test_x, train_y_first, test_y = train_test_split(x, y, test_size = 0.1, random_state = 42)\n",
    "    train_x, train_x2, train_y, train_y2 = train_test_split(train_x_first, train_y_first, test_size = 0.4, shuffle=False)   #训练集中的60%用于模型训练\n",
    "\n",
    "\n",
    "    # 模型输入参数，需要自己根据需要调整\n",
    "    n_units = 100\n",
    "    batch_size = 32\n",
    "    #epochs = 1\n",
    "    output_dim = 20\n",
    "\n",
    "    # 模型训练  进行三轮训练后\n",
    "    lstm_model = create_LSTM(n_units, input_shape, output_dim, filepath)\n",
    "    lstm_model.fit(train_x, train_y, epochs=3, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # 模型保存\n",
    "    lstm_model.save(model_save_path)\n",
    "\n",
    "    N = test_x.shape[0]  # 测试的条数\n",
    "    predict = []\n",
    "    label = []\n",
    "    for start, end in zip(range(0, N, 1), range(1, N+1, 1)):\n",
    "        sentence = [inverse_word_dictionary[i] for i in test_x[start] if i != 0]\n",
    "        y_predict = lstm_model.predict(test_x[start:end])\n",
    "        label_predict = output_dictionary[np.argmax(y_predict[0])]\n",
    "        label_true = output_dictionary[np.argmax(test_y[start:end])]\n",
    "        #print(''.join(sentence), label_true, label_predict) # 输出预测结果\n",
    "        predict.append(label_predict)\n",
    "        label.append(label_true)\n",
    "\n",
    "    acc = accuracy_score(predict, label) # 预测准确率\n",
    "    print('新数据学习前模型在测试集上的准确率为: %s.' % acc)\n",
    "    train_x, train_x3, train_y, train_y3 = train_test_split(train_x2, train_y2, test_size = 0.6, shuffle=False)  #testsize=不筛选引入的新数据学习量  将剩余训练集中的40%视为为人工标记样本进行学习\n",
    "    lstm_model.fit(train_x, train_y, epochs=2, batch_size=batch_size, verbose=1)\n",
    "    lstm_model.save(model_save_path)  #保存\n",
    "\n",
    "    N = test_x.shape[0]  # 测试的条数\n",
    "    predict = []\n",
    "    label = []\n",
    "    for start, end in zip(range(0, N, 1), range(1, N+1, 1)):\n",
    "        sentence = [inverse_word_dictionary[i] for i in test_x[start] if i != 0]\n",
    "        y_predict = lstm_model.predict(test_x[start:end])\n",
    "        label_predict = output_dictionary[np.argmax(y_predict[0])]\n",
    "        label_true = output_dictionary[np.argmax(test_y[start:end])]\n",
    "        #print(''.join(sentence), label_true, label_predict) # 输出预测结果\n",
    "        predict.append(label_predict)\n",
    "        label.append(label_true)\n",
    "    acc = accuracy_score(predict, label) # 预测准确率\n",
    "    print('新数据学习后模型在测试集上的准确率为: %s.' % acc)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filepath = './all_data.csv'\n",
    "    input_shape = 180\n",
    "    model_save_path = './all_data.h5'\n",
    "    model_train(input_shape, filepath, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "284c52679f6741a6bf374bc07498e87d2e564458073e239588a542cbf66792a6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('hzc': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
