{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 导入数据\n",
    "# 文件的数据中，特征为evaluation, 类别为label.\n",
    "def load_data(filepath, input_shape=20):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # 标签及词汇表\n",
    "    labels, vocabulary = list(df['label'].unique()), list(df['evaluation'].unique())\n",
    "\n",
    "    # 构造字符级别的特征\n",
    "    string = ''\n",
    "    for word in vocabulary:\n",
    "        string += word\n",
    "\n",
    "    vocabulary = set(string)\n",
    "\n",
    "    # 字典列表\n",
    "    word_dictionary = {word: i+1 for i, word in enumerate(vocabulary)}\n",
    "    with open('word_dict.pk', 'wb') as f:\n",
    "        pickle.dump(word_dictionary, f)\n",
    "    inverse_word_dictionary = {i+1: word for i, word in enumerate(vocabulary)}\n",
    "    label_dictionary = {label: i for i, label in enumerate(labels)}\n",
    "    with open('label_dict.pk', 'wb') as f:\n",
    "        pickle.dump(label_dictionary, f)\n",
    "    output_dictionary = {i: labels for i, labels in enumerate(labels)}\n",
    "\n",
    "    vocab_size = len(word_dictionary.keys()) # 词汇表大小\n",
    "    label_size = len(label_dictionary.keys()) # 标签类别数量\n",
    "\n",
    "    # 序列填充，按input_shape填充，长度不足的按0补充\n",
    "    x = [[word_dictionary[word] for word in sent] for sent in df['evaluation']]\n",
    "    x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
    "    y = [[label_dictionary[sent]] for sent in df['label']]\n",
    "    y = [np_utils.to_categorical(label, num_classes=label_size) for label in y]\n",
    "    y = np.array([list(_[0]) for _ in y])\n",
    "\n",
    "    return x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary\n",
    "\n",
    "# 创建深度学习模型， Embedding + LSTM + Softmax.\n",
    "def create_LSTM(n_units, input_shape, output_dim, filepath):\n",
    "    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size + 1, output_dim=output_dim,\n",
    "                        input_length=input_shape, mask_zero=True))\n",
    "    model.add(LSTM(n_units, input_shape=(x.shape[0], x.shape[1])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(label_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    plot_model(model, to_file='./model_lstm.png', show_shapes=True)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# 模型训练\n",
    "def model_train(input_shape, filepath, model_save_path):\n",
    "    # 原始数据集分为最终的测试集和主动学习中所用的训练集\n",
    "    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath, input_shape)\n",
    "    train_x_first, test_x_final, train_y_first, test_y_final = train_test_split(x, y, test_size = 0.1, random_state = 42) \n",
    "    # 分离出原始数据的10%作为标准测验集，即test_x_final,test_y_final\n",
    "    train_x, test_x, train_y, test_y = train_test_split(train_x_first, train_y_first, test_size = 0.4, shuffle=False)   \n",
    "    # 训练集first中的60%用于初始模型训练，并将剩下的40%作为调整测试集，在后续该测试集部分会作为主动学习的素材使用，正常情况下该部分应是未标注数据集，需要人工进行标注加工。\n",
    "\n",
    "    # 将数据集分为训练集和测试集，占比为6:4\n",
    "    # 模型输入参数，需要自己根据需要调整\n",
    "\n",
    "    n_units = 100\n",
    "    batch_size = 32\n",
    "    #epochs = 1\n",
    "    output_dim = 20\n",
    "\n",
    "    # 模型训练\n",
    "    lstm_model = create_LSTM(n_units, input_shape, output_dim, filepath)\n",
    "    lstm_model.fit(train_x, train_y, epochs=3, batch_size=batch_size, verbose=1)\n",
    "    # 初始模型训练完成\n",
    "    # 模型保存\n",
    "    lstm_model.save(model_save_path)\n",
    "\n",
    "    # 这里先使用标准测试集对未经过主动学习的模型进行测试，用于对比测试结果\n",
    "    N = test_x_final.shape[0]  # 标准测试集的大小\n",
    "    predict = []               # 预测结果\n",
    "    label = []                 # 人工标注的真实标签\n",
    "    for start, end in zip(range(0, N, 1), range(1, N+1, 1)):\n",
    "        sentence = [inverse_word_dictionary[i] for i in test_x_final[start] if i != 0]\n",
    "        y_predict = lstm_model.predict(test_x_final[start:end])\n",
    "        # 生成预测结果  \n",
    "        label_predict = output_dictionary[np.argmax(y_predict[0])]\n",
    "        label_true = output_dictionary[np.argmax(test_y_final[start:end])]\n",
    "        #print(''.join(sentence), label_true, label_predict) # 输出预测结果\n",
    "        predict.append(label_predict)\n",
    "        label.append(label_true)\n",
    "\n",
    "    acc = accuracy_score(predict, label) # 预测准确率\n",
    "    print('模型在主动学习之前在测试集上的准确率为: %s.' % acc)\n",
    "\n",
    "\n",
    "    #主动学习\n",
    "    \n",
    "    N = test_x.shape[0]  # 测试集的条数 这里的测试集是调整测试集\n",
    "    predict = []         # 预测结果   \n",
    "    label = []           # 人工标注的真实标签\n",
    "    first_predict=[]     # 添加一个预测准确度排序用于辅助排序\n",
    "    for start, end in zip(range(0, N, 1), range(1, N+1, 1)):\n",
    "        sentence = [inverse_word_dictionary[i] for i in test_x[start] if i != 0]\n",
    "        y_predict = lstm_model.predict(test_x[start:end])\n",
    "        #print(y_predict)\n",
    "        first_predict.append(abs(y_predict[0][0]-0.5))         #y_predict输出的是[[true概率，false概率]，数据类型]，取true （概率-0.5）的绝对值来判断置信程度\n",
    "        label_predict = output_dictionary[np.argmax(y_predict[0])]\n",
    "        label_true = output_dictionary[np.argmax(test_y[start:end])]\n",
    "        #print(''.join(sentence), label_true, label_predict) # 输出预测结果\n",
    "        predict.append(label_predict)\n",
    "        label.append(label_true)\n",
    "    #newdict 用来对置信度和对应得测试集进行排序，到test_x1,test_y1 输出的是置信度从高到底的测试集\n",
    "    \n",
    "    newdict=[]\n",
    "    newdict=dict(zip(\n",
    "    [letter for letter in range(len(first_predict))],\n",
    "    [letter for letter in first_predict]\n",
    "    ))\n",
    "    newdict=sorted(newdict.items(),key=lambda x:x[1], reverse=True)\n",
    "    #print(newdict)\n",
    "    idr1=[]\n",
    "    list_idr=[]\n",
    "    for i in newdict:\n",
    "      idr1.append(i[0])\n",
    "    for i in range(len(idr1)):\n",
    "      list_idr.append(idr1.index(i))\n",
    "    #生成排序后的测试集，重新加入测试集\n",
    "    #print(list_idr)\n",
    "    test_x1=test_x[list_idr]\n",
    "    # 应该在这里实现对train_y 的标记
    "    test_y1=test_y[list_idr]\n",
    "    #下面颠倒位置实现，取出前多少条，做训练集，然后剩下的做测试集\n",
    "    test_x, train_x, test_y, train_y = train_test_split(test_x1,test_y1, test_size = 0.4,shuffle=False)\n",
    "    lstm_model.fit(train_x, train_y, epochs=2, batch_size=batch_size, verbose=1)\n",
    "    lstm_model.save(model_save_path)\n",
    "    \n",
    "    # # #下面是重复过程\n",
    "    # N = test_x.shape[0]  # 测试的条数\n",
    "    # predict = []\n",
    "    # label = []\n",
    "    # first_predict=[] #添加一个预测准确度排序\n",
    "    # for start, end in zip(range(0, N, 1), range(1, N+1, 1)):\n",
    "    #     sentence = [inverse_word_dictionary[i] for i in test_x[start] if i != 0]\n",
    "    #     y_predict = lstm_model.predict(test_x[start:end])\n",
    "    #     #print(y_predict)\n",
    "    #     first_predict.append(abs(y_predict[0][0]-0.5))\n",
    "    #     label_predict = output_dictionary[np.argmax(y_predict[0])]\n",
    "    #     label_true = output_dictionary[np.argmax(test_y[start:end])]\n",
    "    #     #print(''.join(sentence), label_true, label_predict) # 输出预测结果\n",
    "    #     predict.append(label_predict)\n",
    "    #     label.append(label_true)\n",
    "    # newdict=[]\n",
    "    # newdict=dict(zip(\n",
    "    # [letter for letter in range(len(first_predict))],\n",
    "    # [letter for letter in first_predict]\n",
    "    # ))\n",
    "    # newdict=sorted(newdict.items(),key=lambda x:x[1], reverse=True)\n",
    "    # #print(newdict)\n",
    "    # idr1=[]\n",
    "    # list_idr=[]\n",
    "    # for i in newdict:\n",
    "    #   idr1.append(i[0])\n",
    "    # for i in range(len(idr1)):\n",
    "    #   list_idr.append(idr1.index(i))\n",
    "    # #print(list_idr)\n",
    "    # test_x1=test_x[list_idr]\n",
    "    # test_y1=test_y[list_idr]\n",
    "    # #取出前多少条，做训练集，然后重新随机筛选做测试集\n",
    "    # test_x, train_x, test_y, train_y = train_test_split(test_x1,test_y1, test_size = 0.25,shuffle=False)\n",
    "    # lstm_model.fit(train_x, train_y, epochs=1, batch_size=batch_size, verbose=1)\n",
    "    # lstm_model.save(model_save_path)\n",
    "    \n",
    "    # N = test_x.shape[0]  # 测试的条数\n",
    "    # predict = []\n",
    "    # label = []\n",
    "    # first_predict=[] #添加一个预测准确度排序\n",
    "    # for start, end in zip(range(0, N, 1), range(1, N+1, 1)):\n",
    "    #     sentence = [inverse_word_dictionary[i] for i in test_x[start] if i != 0]\n",
    "    #     y_predict = lstm_model.predict(test_x[start:end])\n",
    "    #     #print(y_predict)\n",
    "    #     first_predict.append(abs(y_predict[0][0]-0.5))\n",
    "    #     label_predict = output_dictionary[np.argmax(y_predict[0])]\n",
    "    #     label_true = output_dictionary[np.argmax(test_y[start:end])]\n",
    "    #     #print(''.join(sentence), label_true, label_predict) # 输出预测结果\n",
    "    #     predict.append(label_predict)\n",
    "    #     label.append(label_true)\n",
    "    # newdict=[]\n",
    "    # newdict=dict(zip(\n",
    "    # [letter for letter in range(len(first_predict))],\n",
    "    # [letter for letter in first_predict]\n",
    "    # ))\n",
    "    # newdict=sorted(newdict.items(),key=lambda x:x[1], reverse=True)\n",
    "    # #print(newdict)\n",
    "    # idr1=[]\n",
    "    # list_idr=[]\n",
    "    # for i in newdict:\n",
    "    #   idr1.append(i[0])\n",
    "    # for i in range(len(idr1)):\n",
    "    #   list_idr.append(idr1.index(i))\n",
    "    # #print(list_idr)\n",
    "    # test_x1=test_x[list_idr]\n",
    "    # test_y1=test_y[list_idr]\n",
    "    # #取出前多少条，做训练集，然后重新随机筛选做测试集\n",
    "    # test_x, train_x, test_y, train_y = train_test_split(test_x1,test_y1, test_size = 0.4,shuffle=False)\n",
    "    # lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    # lstm_model.save(model_save_path)\n",
    "\n",
    "    \n",
    "    #用初始分开的测试集进行结果测试\n",
    "    N = test_x_final.shape[0]  # 测试的条数\n",
    "    predict = []\n",
    "    label = []\n",
    "    for start, end in zip(range(0, N, 1), range(1, N+1, 1)):\n",
    "        sentence = [inverse_word_dictionary[i] for i in test_x_final[start] if i != 0]\n",
    "        y_predict = lstm_model.predict(test_x_final[start:end])\n",
    "        label_predict = output_dictionary[np.argmax(y_predict[0])]\n",
    "        label_true = output_dictionary[np.argmax(test_y_final[start:end])]\n",
    "        #print(''.join(sentence), label_true, label_predict) # 输出预测结果\n",
    "        predict.append(label_predict)\n",
    "        label.append(label_true)\n",
    "\n",
    "    acc = accuracy_score(predict, label) # 预测准确率\n",
    "    print('模型在主动学习之后在测试集上的准确率为: %s.' % acc)\n",
    "    #acc = accuracy_score(predict, label) # 预测准确率\n",
    "    #print('模型在测试集上的准确率为: %s.' % acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 180, 20)           50940     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 99,542\n",
      "Trainable params: 99,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "191/191 [==============================] - 17s 79ms/step - loss: 0.3927 - accuracy: 0.8407\n",
      "Epoch 2/3\n",
      "191/191 [==============================] - 16s 82ms/step - loss: 0.2315 - accuracy: 0.9257\n",
      "Epoch 3/3\n",
      "191/191 [==============================] - 16s 84ms/step - loss: 0.1991 - accuracy: 0.9376\n",
      "模型在主动学习之前在测试集上的准确率为: 0.8913427561837456.\n",
      "Epoch 1/2\n",
      "51/51 [==============================] - 5s 90ms/step - loss: 0.2886 - accuracy: 0.9067\n",
      "Epoch 2/2\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.2125 - accuracy: 0.9294\n",
      "模型在主动学习之后在测试集上的准确率为: 0.9098939929328622.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    filepath = './all_data.csv'\n",
    "    input_shape = 180\n",
    "    model_save_path = './all_data1.h5'\n",
    "    model_train(input_shape, filepath, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
